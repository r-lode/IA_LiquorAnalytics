{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: [Sales Data from Iowa Class “E” liquor licensees](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data) from January 2012 through the end of September 2023. Downloaded November 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1058/2407653461.py:1: DtypeWarning: Columns (6,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  liquor_data = pd.read_csv('Iowa_Liquor_Sales.csv')\n"
     ]
    }
   ],
   "source": [
    "liquor_data = pd.read_csv('Iowa_Liquor_Sales.csv')\n",
    "\n",
    "#drop liquor invoice column\n",
    "liquor_data = liquor_data.drop(liquor_data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in that data took a long time. A further complication is that a number of columns have mixed data types. Let's do some further investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset: \n",
      "27489743\n",
      "Number of missing values: \n",
      "5688292\n",
      "Percentage of missing values: \n",
      "20.692416076789076\n"
     ]
    }
   ],
   "source": [
    "print('Length of Dataset: ')\n",
    "print(len(liquor_data))\n",
    "print('Number of missing values: ')\n",
    "num_missing_values = (liquor_data.isnull().sum(axis=1) > 0).sum()\n",
    "print(num_missing_values)\n",
    "print('Percentage of missing values: ')\n",
    "print(num_missing_values/len(liquor_data) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a very large number of rows with missing values, accounting for over 20 percent of the total data. Extensive cleaning will be required before this data will be workable. \n",
    "\n",
    "Because the file is so large, it is almost impossible to work with in its entirety. At least on my computer...Therefore, I decided to split it into more digestable chunks based on the year. The data begins in 2012 and ends in 2023, so in the end, I want 12 dataframes corresponding to each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes:\n",
      "Iowa_Liquor_Data_2012: 2082059 rows\n",
      "Iowa_Liquor_Data_2013: 2063763 rows\n",
      "Iowa_Liquor_Data_2014: 2097796 rows\n",
      "Iowa_Liquor_Data_2015: 2184483 rows\n",
      "Iowa_Liquor_Data_2016: 2279893 rows\n",
      "Iowa_Liquor_Data_2017: 2291276 rows\n",
      "Iowa_Liquor_Data_2018: 2355558 rows\n",
      "Iowa_Liquor_Data_2019: 2380345 rows\n",
      "Iowa_Liquor_Data_2020: 2614365 rows\n",
      "Iowa_Liquor_Data_2021: 2622712 rows\n",
      "Iowa_Liquor_Data_2022: 2564565 rows\n",
      "Iowa_Liquor_Data_2023: 1952928 rows\n"
     ]
    }
   ],
   "source": [
    "start_year = 2012\n",
    "end_year = 2023\n",
    "\n",
    "\n",
    "#initialize row count\n",
    "total_rows = 0\n",
    "\n",
    "#dictionary to store dataframes by year\n",
    "dataframes_by_year = {}\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "\n",
    "    #create a mask to filter for current year\n",
    "    mask = liquor_data['Date'].str.contains(str(year))\n",
    "\n",
    "    #instantiate dataframe for current year\n",
    "    year_data = liquor_data[mask].copy()\n",
    "    total_rows += len(year_data)\n",
    "\n",
    "    #define a dataframe for the current year\n",
    "    year_dataframe_name = f'Iowa_Liquor_Data_{year}'\n",
    "\n",
    "    #store dataframe in dictionary\n",
    "    dataframes_by_year[year_dataframe_name] = year_data\n",
    "\n",
    "\n",
    "# Access the dataframes using the dictionary\n",
    "print(\"Dataframes:\")\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"{name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kept a variable that tracks the number of rows in each yearly dataframe. Let's make sure we didn't lose any data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows across all dataframes: \n",
      "27489743\n"
     ]
    }
   ],
   "source": [
    "print('Total rows across all dataframes: ')\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing missing data will be easier if we can separate all of the rows containing missing data into a single dataframe. However, we will need to remove those rows from their original, yearly dataframe to avoid adding duplicates if we add them back in. However, there is a problem. The 2023 data's 'County Number' column is entirely missing, meaning any filtering that removes rows based on the presence of NA values won't be helpful for the year of 2023. \n",
    "\n",
    "Additionally, there are large amounts of other rows containing either missing 'County' values or missing 'County Number' values. Let's loop through our yearly dataframes and find out how many rows contain either a missing 'County' value or a missing 'County Number' but not both (XOR). We will also print the counts for each dataframe where this condition is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing county data from Iowa_Liquor_Data_2012 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2013 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2014 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2015 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2016 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2017 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2018 : 2\n",
      "Count of missing county data from Iowa_Liquor_Data_2019 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2020 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2021 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2022 : 1245255\n",
      "Count of missing county data from Iowa_Liquor_Data_2023 : 1952127\n"
     ]
    }
   ],
   "source": [
    "#dictionary to store the missing county data counts for each year\n",
    "counts_by_year = {}\n",
    "\n",
    "#iterate through yearly dataframes\n",
    "for year, df in dataframes_by_year.items():\n",
    "    #get num rows satisfying missing county condition\n",
    "    count_condition = (df['County'].isnull() ^ df['County Number'].isnull())\n",
    "    count = count_condition.sum()  # Count the number of True values\n",
    "\n",
    "    #store count\n",
    "    counts_by_year[year] = count\n",
    "\n",
    "    #output ccount\n",
    "    print(f\"Count of missing county data from {year} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a tiny sliver of missing county data occurs in 2018, while the vast majority occurs in either 2022 or 2023. Investigating those 2018 rows reveals that two rows are from non-Iowa cities. We will remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Store Number</th>\n",
       "      <th>Store Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Store Location</th>\n",
       "      <th>County Number</th>\n",
       "      <th>County</th>\n",
       "      <th>Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Item Number</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Pack</th>\n",
       "      <th>Bottle Volume (ml)</th>\n",
       "      <th>State Bottle Cost</th>\n",
       "      <th>State Bottle Retail</th>\n",
       "      <th>Bottles Sold</th>\n",
       "      <th>Sale (Dollars)</th>\n",
       "      <th>Volume Sold (Liters)</th>\n",
       "      <th>Volume Sold (Gallons)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7983617</th>\n",
       "      <td>01/18/2018</td>\n",
       "      <td>9936</td>\n",
       "      <td>DASH EVENTS LLC</td>\n",
       "      <td>1685 W UINTAH ST. #101</td>\n",
       "      <td>COLORADO SPRINGS</td>\n",
       "      <td>80904.0</td>\n",
       "      <td>POINT (-104.845334 38.848017)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EL PASO</td>\n",
       "      <td>1081200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78592</td>\n",
       "      <td>ADELAIDES CARAMEL MOCHA MOJO</td>\n",
       "      <td>12</td>\n",
       "      <td>750</td>\n",
       "      <td>10.38</td>\n",
       "      <td>15.57</td>\n",
       "      <td>6</td>\n",
       "      <td>15.57</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8002556</th>\n",
       "      <td>01/18/2018</td>\n",
       "      <td>9936</td>\n",
       "      <td>DASH EVENTS LLC</td>\n",
       "      <td>1685 W UINTAH ST. #101</td>\n",
       "      <td>COLORADO SPRINGS</td>\n",
       "      <td>80904.0</td>\n",
       "      <td>POINT (-104.845334 38.848017)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EL PASO</td>\n",
       "      <td>1081300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71412</td>\n",
       "      <td>ADELAIDES COCONUT LIQUEUR</td>\n",
       "      <td>12</td>\n",
       "      <td>750</td>\n",
       "      <td>10.38</td>\n",
       "      <td>15.57</td>\n",
       "      <td>6</td>\n",
       "      <td>15.57</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date  Store Number       Store Name                 Address  \\\n",
       "7983617  01/18/2018          9936  DASH EVENTS LLC  1685 W UINTAH ST. #101   \n",
       "8002556  01/18/2018          9936  DASH EVENTS LLC  1685 W UINTAH ST. #101   \n",
       "\n",
       "                     City Zip Code                 Store Location  \\\n",
       "7983617  COLORADO SPRINGS  80904.0  POINT (-104.845334 38.848017)   \n",
       "8002556  COLORADO SPRINGS  80904.0  POINT (-104.845334 38.848017)   \n",
       "\n",
       "         County Number   County   Category  ... Item Number  \\\n",
       "7983617            NaN  EL PASO  1081200.0  ...       78592   \n",
       "8002556            NaN  EL PASO  1081300.0  ...       71412   \n",
       "\n",
       "                     Item Description Pack Bottle Volume (ml)  \\\n",
       "7983617  ADELAIDES CARAMEL MOCHA MOJO   12                750   \n",
       "8002556     ADELAIDES COCONUT LIQUEUR   12                750   \n",
       "\n",
       "        State Bottle Cost  State Bottle Retail  Bottles Sold  Sale (Dollars)  \\\n",
       "7983617             10.38                15.57             6           15.57   \n",
       "8002556             10.38                15.57             6           15.57   \n",
       "\n",
       "         Volume Sold (Liters)  Volume Sold (Gallons)  \n",
       "7983617                   4.5                   1.18  \n",
       "8002556                   4.5                   1.18  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liquor_2018 = dataframes_by_year['Iowa_Liquor_Data_2018']\n",
    "\n",
    "# Filter rows where the XOR condition holds for 'County' and 'County Number' columns\n",
    "xor_condition_rows_2018 = liquor_2018[(liquor_2018['County'].isnull() ^ liquor_2018['County Number'].isnull())]\n",
    "\n",
    "xor_condition_rows_2018.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bad 2018 data\n",
    "xor_condition_rows_2018 = dataframes_by_year['Iowa_Liquor_Data_2018'][\n",
    "    (dataframes_by_year['Iowa_Liquor_Data_2018']['County'].isnull() ^\n",
    "     dataframes_by_year['Iowa_Liquor_Data_2018']['County Number'].isnull())\n",
    "]\n",
    "\n",
    "#get indicies of bad rows\n",
    "rows_to_remove_indices = xor_condition_rows_2018.index\n",
    "\n",
    "#using indicies, remove the rows from the 2018 DataFrame in dataframes_by_year\n",
    "dataframes_by_year['Iowa_Liquor_Data_2018'] = dataframes_by_year['Iowa_Liquor_Data_2018'].drop(rows_to_remove_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we return to the other missing county data. As we can see, missing county data occurs in only the 2022 and 2023 years, but rows with missing county data represent over 50% of the total rows with missing values. Therefore, it will be crucial to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5620989787792113\n"
     ]
    }
   ],
   "source": [
    "total_missing_county_data =  1245255 + 1952127\n",
    "print(total_missing_county_data / (num_missing_values - 2)) #subtracting 2 since we removed two rows with missing vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill in this missing data, let's first extract  Iowa County and Iowa County Numbers from this pdf taken here: https://tax.iowa.gov/sites/default/files/2020-07/Iowa%20County%20Names%20and%20Numbers.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    # Create a PdfReader object\n",
    "    pdf = PdfReader(pdf_path)\n",
    "\n",
    "    county_numbers = []\n",
    "    county_names = []\n",
    "\n",
    "    # Extract text from the first page of the PDF\n",
    "    text = pdf.pages[0].extract_text()\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        if '-' in line and '76-002' not in line:  # Exclude line with '76-002'\n",
    "            county_number, county_name = line.split('-', 1)\n",
    "            county_numbers.append(int(county_number.strip()))  # Convert to int64\n",
    "            county_names.append(county_name.strip().upper())  # Convert to uppercase\n",
    "        elif 'O’BRIEN' in line:\n",
    "            parts = line.split('\\t')\n",
    "            county_numbers.append(parts[0].strip())\n",
    "            county_names.append(parts[1].strip().upper())\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'County Number': county_numbers,\n",
    "        'County': county_names\n",
    "    })\n",
    "\n",
    "    # Delete rows where 'County Number' equals 33, 66, and 71\n",
    "    df = df[~df['County Number'].isin([33, 66, 71])]\n",
    "\n",
    "    # Manually set new entries for 33, 66, and 71\n",
    "    new_entries = [\n",
    "        {'County Number': 33, 'County': 'FAYETTE'},\n",
    "        {'County Number': 34, 'County': 'FLOYD'},\n",
    "        {'County Number': 66, 'County': 'MITCHELL'},\n",
    "        {'County Number': 67, 'County': 'MONONA'},\n",
    "        {'County Number': 71, 'County': \"O'BRIEN\"}\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame from the new entries\n",
    "    new_entries_df = pd.DataFrame(new_entries)\n",
    "\n",
    "    # Concatenate the original DataFrame and the new entries DataFrame\n",
    "    df = pd.concat([df, new_entries_df], ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame based on 'County Number' column in ascending order\n",
    "    df = df.sort_values('County Number', ascending=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file path\n",
    "path = 'Iowa County Names and Numbers.pdf'\n",
    "#instantiate df countaining county data\n",
    "county_df = extract_data_from_pdf(path)\n",
    "#output data to csv if desired\n",
    "#county_df.to_csv('counties.csv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through the dataframes containing missing county data (those with either county or county number missing, not both), and use the paired county information in each row to fill in the relevant missing data. We start by creating two dictionaries for county number and county. Then, we iterate through each dataframe and check whether the year is 2022 or 2023, since these dfs contain the relevant rows with missing county data. Then, we check if either the missing value is county or county number and use that to fill in the missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping DataFrame 'Iowa_Liquor_Data_2012' for year 2012.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2013' for year 2013.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2014' for year 2014.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2015' for year 2015.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2016' for year 2016.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2017' for year 2017.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2018' for year 2018.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2019' for year 2019.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2020' for year 2020.\n",
      "Skipping DataFrame 'Iowa_Liquor_Data_2021' for year 2021.\n",
      "Processing DataFrame 'Iowa_Liquor_Data_2022' completed for year 2022.\n",
      "Processing DataFrame 'Iowa_Liquor_Data_2023' completed for year 2023.\n"
     ]
    }
   ],
   "source": [
    "#creating dictionarie for quick lookup of relevant county or county number data.\n",
    "county_number_to_county = county_df.set_index('County Number')['County'].to_dict()\n",
    "county_to_county_number = county_df.set_index('County')['County Number'].to_dict()\n",
    "\n",
    "#get list of dataframes\n",
    "dataframes_names = list(dataframes_by_year)\n",
    "\n",
    "#iterate through every year in yearly dataframes\n",
    "for dataframe_name in dataframes_names:\n",
    "    #get year of dataframe\n",
    "    year = int(dataframe_name[-4:])  #\n",
    "\n",
    "    #check whether year is 2022 or 2023 to speed up process\n",
    "    if year in [2022, 2023]:\n",
    "        df = dataframes_by_year[dataframe_name]\n",
    "        for index, row in df.iterrows():\n",
    "            #check whether row meets XOR criteria\n",
    "            if pd.isnull(row['County']) ^ pd.isnull(row['County Number']):\n",
    "                #if county is missing, fill it using county number from the dictionary\n",
    "                if pd.isnull(row['County']):\n",
    "                    county_number = row['County Number']\n",
    "                    if county_number in county_number_to_county:\n",
    "                        df.at[index, 'County'] = county_number_to_county[county_number]\n",
    "                #if county number is missing, fill it using county from the dictionary\n",
    "                else:\n",
    "                    county_name = row['County']\n",
    "                    if county_name in county_to_county_number:\n",
    "                        df.at[index, 'County Number'] = county_to_county_number[county_name]\n",
    "\n",
    "        print(f\"Processing DataFrame '{dataframe_name}' completed for year {year}.\")\n",
    "    else:\n",
    "        print(f\"Skipping DataFrame '{dataframe_name}' for year {year}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun this earlier block of code to double check that we successfully filled in missing county data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing county data from Iowa_Liquor_Data_2012 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2013 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2014 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2015 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2016 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2017 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2018 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2019 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2020 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2021 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2022 : 0\n",
      "Count of missing county data from Iowa_Liquor_Data_2023 : 0\n"
     ]
    }
   ],
   "source": [
    "#dictionary to store the missing county data counts for each year\n",
    "counts_by_year = {}\n",
    "\n",
    "#iterate through yearly dataframes\n",
    "for year, df in dataframes_by_year.items():\n",
    "    #get num rows satisfying missing county condition\n",
    "    count_condition = (df['County'].isnull() ^ df['County Number'].isnull())\n",
    "    count = count_condition.sum()  # Count the number of True values\n",
    "\n",
    "    #store count\n",
    "    counts_by_year[year] = count\n",
    "\n",
    "    #output ccount\n",
    "    print(f\"Count of missing county data from {year} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it came to my attention that a number of city names in the original dataset were misspelled. Let's identify them. We will do this by downloading the Iowa Populated Places dataset from here: https://geodata.iowa.gov/datasets/iowa-populated-places/explore \n",
    "\n",
    "We will also need to download the Iowa Liquor Stores Dataset for reasons that will become clear later on. This can be found here: https://data.iowa.gov/Regulation/Iowa-Liquor-Stores/ykb6-ywnd \n",
    "\n",
    "Column names in the liquor_stores dataframe are adjusted for easier use later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>FID</th>\n",
       "      <th>PLACES_ID</th>\n",
       "      <th>PLACE_NAME</th>\n",
       "      <th>NAME_CAPS</th>\n",
       "      <th>OTHER_NAME</th>\n",
       "      <th>CO_NAME</th>\n",
       "      <th>CO_SEAT</th>\n",
       "      <th>QUAD_NAME</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>CO_NUMBER</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>typedescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-96.556444</td>\n",
       "      <td>43.468872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Granite</td>\n",
       "      <td>GRANITE</td>\n",
       "      <td></td>\n",
       "      <td>LYON</td>\n",
       "      <td></td>\n",
       "      <td>KLONDIKE</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>-96.556444</td>\n",
       "      <td>43.468872</td>\n",
       "      <td>Unincorporated town or place name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-96.100309</td>\n",
       "      <td>43.476366</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Midland</td>\n",
       "      <td>MIDLAND</td>\n",
       "      <td></td>\n",
       "      <td>LYON</td>\n",
       "      <td></td>\n",
       "      <td>EDNA</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>-96.100309</td>\n",
       "      <td>43.476366</td>\n",
       "      <td>GNIS place name not referenced by USGS or IDOT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-91.289585</td>\n",
       "      <td>43.496649</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>New Albin</td>\n",
       "      <td>NEW ALBIN</td>\n",
       "      <td></td>\n",
       "      <td>ALLAMAKEE</td>\n",
       "      <td></td>\n",
       "      <td>NEW ALBIN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-91.289585</td>\n",
       "      <td>43.496649</td>\n",
       "      <td>Incorporated city or town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-94.791935</td>\n",
       "      <td>43.491636</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Huntington</td>\n",
       "      <td>HUNTINGTON</td>\n",
       "      <td></td>\n",
       "      <td>EMMET</td>\n",
       "      <td></td>\n",
       "      <td>ESTHERVILLE</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>-94.791935</td>\n",
       "      <td>43.491636</td>\n",
       "      <td>Unincorporated town or place name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-96.434216</td>\n",
       "      <td>43.453596</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Larchwood</td>\n",
       "      <td>LARCHWOOD</td>\n",
       "      <td></td>\n",
       "      <td>LYON</td>\n",
       "      <td></td>\n",
       "      <td>LARCHWOOD</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>-96.434216</td>\n",
       "      <td>43.453596</td>\n",
       "      <td>Incorporated city or town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          Y  FID  PLACES_ID  PLACE_NAME   NAME_CAPS OTHER_NAME  \\\n",
       "0 -96.556444  43.468872    1          1     Granite     GRANITE              \n",
       "1 -96.100309  43.476366    2          2     Midland     MIDLAND              \n",
       "2 -91.289585  43.496649    3          3   New Albin   NEW ALBIN              \n",
       "3 -94.791935  43.491636    4          4  Huntington  HUNTINGTON              \n",
       "4 -96.434216  43.453596    5          5   Larchwood   LARCHWOOD              \n",
       "\n",
       "     CO_NAME CO_SEAT    QUAD_NAME  TYPE  CO_NUMBER  Longitude   Latitude  \\\n",
       "0       LYON             KLONDIKE     3         60 -96.556444  43.468872   \n",
       "1       LYON                 EDNA    10         60 -96.100309  43.476366   \n",
       "2  ALLAMAKEE            NEW ALBIN     1          3 -91.289585  43.496649   \n",
       "3      EMMET          ESTHERVILLE     3         32 -94.791935  43.491636   \n",
       "4       LYON            LARCHWOOD     1         60 -96.434216  43.453596   \n",
       "\n",
       "                                     typedescription  \n",
       "0                  Unincorporated town or place name  \n",
       "1  GNIS place name not referenced by USGS or IDOT...  \n",
       "2                          Incorporated city or town  \n",
       "3                  Unincorporated town or place name  \n",
       "4                          Incorporated city or town  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iowa_pop_places = pd.read_csv('Iowa_Populated_Places.csv')\n",
    "iowa_pop_places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store Number</th>\n",
       "      <th>Store Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Store Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10261</td>\n",
       "      <td>AVENUE G STORE / COUNCIL BLUFFS</td>\n",
       "      <td>1602 AVENUE G</td>\n",
       "      <td>COUNCIL BLUFFS</td>\n",
       "      <td>IA</td>\n",
       "      <td>51501</td>\n",
       "      <td>POINT (-95.868007008 41.268345002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4191</td>\n",
       "      <td>FAREWAY STORES #995 / PELLA</td>\n",
       "      <td>2010 WASHINGTON ST</td>\n",
       "      <td>PELLA</td>\n",
       "      <td>IA</td>\n",
       "      <td>50219</td>\n",
       "      <td>POINT (-92.941820991 41.413265006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5268</td>\n",
       "      <td>THE DEPOT ATKINS</td>\n",
       "      <td>188, PARKRIDGE RD</td>\n",
       "      <td>ATKINS</td>\n",
       "      <td>IA</td>\n",
       "      <td>52206</td>\n",
       "      <td>POINT (-91.862706026 41.992304983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5929</td>\n",
       "      <td>D&amp;T'S / ODEBOLT</td>\n",
       "      <td>417 1ST ST</td>\n",
       "      <td>ODEBOLT</td>\n",
       "      <td>IA</td>\n",
       "      <td>51458</td>\n",
       "      <td>POINT (-95.255976028 42.313622014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6150</td>\n",
       "      <td>THE DEPOT GILMAN</td>\n",
       "      <td>102 S. ELM ST.</td>\n",
       "      <td>GILMAN</td>\n",
       "      <td>IA</td>\n",
       "      <td>50106</td>\n",
       "      <td>POINT (-92.785956987 41.879548017)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store Number                       Store Name             Address  \\\n",
       "0         10261  AVENUE G STORE / COUNCIL BLUFFS       1602 AVENUE G   \n",
       "1          4191      FAREWAY STORES #995 / PELLA  2010 WASHINGTON ST   \n",
       "2          5268                 THE DEPOT ATKINS   188, PARKRIDGE RD   \n",
       "3          5929                  D&T'S / ODEBOLT          417 1ST ST   \n",
       "4          6150                 THE DEPOT GILMAN      102 S. ELM ST.   \n",
       "\n",
       "             City State  Zip Code                      Store Location  \n",
       "0  COUNCIL BLUFFS    IA     51501  POINT (-95.868007008 41.268345002)  \n",
       "1           PELLA    IA     50219  POINT (-92.941820991 41.413265006)  \n",
       "2          ATKINS    IA     52206  POINT (-91.862706026 41.992304983)  \n",
       "3         ODEBOLT    IA     51458  POINT (-95.255976028 42.313622014)  \n",
       "4          GILMAN    IA     50106  POINT (-92.785956987 41.879548017)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liquor_stores = pd.read_csv('Iowa_Liquor_Stores.csv')\n",
    "liquor_stores = liquor_stores.drop(['Store Status', 'Report Date'], axis=1)\n",
    "liquor_stores.rename(columns={'Store Address': 'Store Location'}, inplace=True)\n",
    "liquor_stores.rename(columns={'Store': 'Store Number'}, inplace=True)\n",
    "liquor_stores.rename(columns={'Name': 'Store Name'}, inplace=True)\n",
    "liquor_stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by identifying which rows in our liquor_data don't have a corresponding match in the Iowa Populated Places Dataset (i.e. they have been mispelled). We will start by selecting only the city columns of each. In the liquor data, the city column is called 'CITY' and in the 'Iowa Populated Places' Dataset, it is called 'NAME_CAPS'. We will right join on 'CITY to identify rows that have an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get city columns only in liquor data and iowa populated places data\n",
    "liquor_city_data = liquor_data[['City']].copy()\n",
    "iowa_city_data = iowa_pop_places[['NAME_CAPS']].copy()\n",
    "\n",
    "#merge them together\n",
    "merged = iowa_city_data.merge(liquor_city_data, how='right', left_on='NAME_CAPS', right_on='City')\n",
    "\n",
    "#filter rows which don't have a corresponding match\n",
    "rows_without_matches = merged[merged['NAME_CAPS'].isnull()]\n",
    "\n",
    "#how many rows contain mismatches?\n",
    "len(rows_without_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output which City values that are incorrectly spelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "ARNOLDS PARK        86092\n",
       "DEWITT              67796\n",
       "LEMARS              44794\n",
       "OTTUWMA             40246\n",
       "CLEARLAKE           33857\n",
       "MT VERNON           28649\n",
       "MT PLEASANT         22720\n",
       "LAKE VIEW           21598\n",
       "LECLAIRE            19058\n",
       "ST ANSGAR           15684\n",
       "MELCHER-DALLAS      14173\n",
       "ARNOLD'S PARK        8171\n",
       "ST LUCAS             6868\n",
       "ST CHARLES           3546\n",
       "GRAND MOUNDS         2895\n",
       "GUTTENBURG           2593\n",
       "KELLOG               2112\n",
       "OTUMWA               1516\n",
       "LONETREE              851\n",
       "FT. ATKINSON          566\n",
       "COLORADO SPRINGS        2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_without_matches['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things jump out. First of all, there are actually a couple notable errors in the Iowa Populated places dataset. It incorrectly labels ARNOLDS PARK as 'ARNOLD PARK', however, it did highlight that the need to correct 'ARNOLD'S PARK to ARNOLDS PARK. \n",
    "\n",
    "DEWITT is correctly spelled as one word in our original dataset, but not in 'Iowa Populated Places'. \n",
    "\n",
    "LAKE VIEW is also correctly spelled as two words in our liquor dataset, while it is incorrect in the 'Iowa Populated Places' dataset. This begs the question on whether DE WITT and LAKEVIEW also appear in our liquor dataset.  \n",
    "\n",
    "Next, LEMARS, CLEARLAKE, LECLAIRE, LONETREE all need spaces. GUTTENBURG, OTUMWA, OTTUWMA, GRAND MOUNDS, and KELLOG are misspelled and COLORADO SPRINGS is not a city in Iowa, but we already addressed this.\n",
    "\n",
    "Finally we will have to deal with inconsistencies in our abbreviated terms. \n",
    "\n",
    "First, let's check how potentially abbreviated terms appear in our liquor dataset. We will also check whether the incorrectly spelled 'LAKEVIEW' and 'DE WITT' cities appear in our original liquor dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "FORT DODGE         311482\n",
       "FORT MADISON       119470\n",
       "PLEASANT HILL      109047\n",
       "MOUNT VERNON        96708\n",
       "MOUNT PLEASANT      79859\n",
       "MT VERNON           28649\n",
       "MT PLEASANT         22720\n",
       "ST ANSGAR           15684\n",
       "FORT ATKINSON       13556\n",
       "PLEASANTVILLE        9095\n",
       "ST LUCAS             6868\n",
       "AFTON                3531\n",
       "PLEASANT VALLEY      1451\n",
       "SAINT ANSGAR         1393\n",
       "FT. ATKINSON          566\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#values to match in the city column\n",
    "specific_cities = ['VERNON', 'PLEASANT', 'ANSGAR', 'LUCAS', 'FORT', 'FT.', 'SAINT', 'LAKEVIEW', 'DE WITT']\n",
    "\n",
    "#create a mask for rows containing specified values\n",
    "mask = liquor_city_data['City'].str.contains('|'.join(specific_cities), case=False, na=False)\n",
    "\n",
    "#store values city values\n",
    "specific_cities_df = liquor_city_data[mask].copy()\n",
    "\n",
    "specific_cities_df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are inconsistencies in how the same town are spelled in the data. We will need to correct this. To avoid any confusion, we convert 'MT' to 'MOUNT, 'ST' to SAINT', 'FT.' to 'FORT', etc. 'DE WITT' and 'LAKE VIEW' do not appear in our original dataset, which is nice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize here are all of the differnt naming changes that need to happen:\n",
    "\n",
    "ARNOLD'S PARK --> ARNOLDS PARK \\\n",
    "ARNOLD PARK --> ARNOLDS PARK \\\n",
    "OTTUWMA --> OTTUMWA \\\n",
    "OTUMWA --> OTTUMWA \\\n",
    "GUTTENBURG --> GUTTENBERG \\\n",
    "KELLOG --> KELLOGG \\\n",
    "GRAND MOUNDS --> GRAND MOUND \\\n",
    "LEMARS --> LE MARS \\\n",
    "CLEARLAKE --> CLEAR LAKE \\\n",
    "LECLAIRE --> LE CLAIRE \\\n",
    "LONETREE --> LONE TREE \\\n",
    "FT. ATKINSON --> FORT ATKINSON \\\n",
    "MT VERNON --> MOUNT VERNON \\\n",
    "MT PLEASANT --> MOUNT PLEASANT \\\n",
    "ST ANGSAR --> SAINT ANSGAR \n",
    "\n",
    "Now we need to check whether the liquor stores dataset contains mismatches and correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liquor store city data\n",
    "liquor_store_city_data = liquor_stores[['City']].copy()\n",
    "\n",
    "#merge them together\n",
    "merged_liquor_stores = iowa_city_data.merge(liquor_store_city_data, how='right', left_on='NAME_CAPS', right_on='City')\n",
    "\n",
    "#filter rows which don't have a corresponding match\n",
    "liquor_store_cities_without_matches = merged_liquor_stores[merged_liquor_stores['NAME_CAPS'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "DEWITT            7\n",
       "CLEARLAKE         4\n",
       "ARNOLDS PARK      3\n",
       "LECLAIRE          3\n",
       "LAKE VIEW         3\n",
       "ST ANSGAR         2\n",
       "MELCHER-DALLAS    2\n",
       "MT PLEASANT       2\n",
       "BEND              1\n",
       "ST LUCAS          1\n",
       "ARNOLD'S PARK     1\n",
       "ST CHARLES        1\n",
       "GRAND MOUNDS      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liquor_store_cities_without_matches['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are similar inconsistencies in this dataset as well. Let's create a mapping to correct these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping to correct misspelling.\n",
    "city_replacements = {\n",
    "        \"ARNOLD'S PARK\": 'ARNOLDS PARK',\n",
    "        'ARNOLD PARK': 'ARNOLDS PARK',\n",
    "        'OTTUWMA': 'OTTUMWA',\n",
    "        'OTUMWA': 'OTTUMWA',\n",
    "        'GUTTENBURG': 'GUTTENBERG',\n",
    "        'KELLOG': 'KELLOGG',\n",
    "        'GRAND MOUNDS': 'GRAND MOUND',\n",
    "        'LEMARS': 'LE MARS',\n",
    "        'CLEARLAKE': 'CLEAR LAKE',\n",
    "        'LECLAIRE': 'LE CLAIRE',\n",
    "        'LONETREE': 'LONE TREE',\n",
    "        'FT. ATKINSON': 'FORT ATKINSON',\n",
    "        'MT VERNON': 'MOUNT VERNON',\n",
    "        'MT PLEASANT': 'MOUNT PLEASANT',\n",
    "        'ST ANSGAR': 'SAINT ANSGAR',\n",
    "        'ST LUCAS': 'SAINT LUCAS',\n",
    "        'ST CHARLES': 'SAINT CHARLES'\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataFrame: Iowa_Liquor_Data_2012\n",
      "Processing dataFrame: Iowa_Liquor_Data_2013\n",
      "Processing dataFrame: Iowa_Liquor_Data_2014\n",
      "Processing dataFrame: Iowa_Liquor_Data_2015\n",
      "Processing dataFrame: Iowa_Liquor_Data_2016\n",
      "Processing dataFrame: Iowa_Liquor_Data_2017\n",
      "Processing dataFrame: Iowa_Liquor_Data_2018\n",
      "Processing dataFrame: Iowa_Liquor_Data_2019\n",
      "Processing dataFrame: Iowa_Liquor_Data_2020\n",
      "Processing dataFrame: Iowa_Liquor_Data_2021\n",
      "Processing dataFrame: Iowa_Liquor_Data_2022\n",
      "Processing dataFrame: Iowa_Liquor_Data_2023\n"
     ]
    }
   ],
   "source": [
    "#loop through dataframes in df list\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"Processing dataFrame: {name}\")\n",
    "    \n",
    "    #loop to correct misspelling\n",
    "    for incorrect_city, correct_city in city_replacements.items():\n",
    "        df['City'] = df['City'].replace(incorrect_city, correct_city)\n",
    "    \n",
    "    #update yearly dataframe\n",
    "    dataframes_by_year[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update liquor store data\n",
    "for incorrect_city, correct_city in city_replacements.items():\n",
    "    liquor_stores['City'] = liquor_stores['City'].replace(incorrect_city, correct_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect one of the yearly dataframes. Here we can see that Zip Code and Item Number are of type object, indicating that these might have mismatched datatypes, given the prior warnings. We will iterate through the Zip Code and Item Number colums to detect non-numeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2082059 entries, 3815854 to 26568553\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Date                   object \n",
      " 1   Store Number           int64  \n",
      " 2   Store Name             object \n",
      " 3   Address                object \n",
      " 4   City                   object \n",
      " 5   Zip Code               object \n",
      " 6   Store Location         object \n",
      " 7   County Number          float64\n",
      " 8   County                 object \n",
      " 9   Category               float64\n",
      " 10  Category Name          object \n",
      " 11  Vendor Number          float64\n",
      " 12  Vendor Name            object \n",
      " 13  Item Number            object \n",
      " 14  Item Description       object \n",
      " 15  Pack                   int64  \n",
      " 16  Bottle Volume (ml)     int64  \n",
      " 17  State Bottle Cost      float64\n",
      " 18  State Bottle Retail    float64\n",
      " 19  Bottles Sold           int64  \n",
      " 20  Sale (Dollars)         float64\n",
      " 21  Volume Sold (Liters)   float64\n",
      " 22  Volume Sold (Gallons)  float64\n",
      "dtypes: float64(8), int64(4), object(11)\n",
      "memory usage: 381.2+ MB\n"
     ]
    }
   ],
   "source": [
    "liquor_2012 = dataframes_by_year['Iowa_Liquor_Data_2012']\n",
    "liquor_2012.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values detected in 'Zip Code' column in yearly dataframes (excluding NA values):\n"
     ]
    }
   ],
   "source": [
    "#regex pattern to find rows with non-numeric values(excudling '.')\n",
    "pattern = r'[^0-9\\.]'  \n",
    "\n",
    "#list to store dfs with non_numeric values\n",
    "dataframes_with_non_numeric_values = []\n",
    "\n",
    "#loop to iterate through yearly dataframes over 'Zip Code'\n",
    "for df_name, df in dataframes_by_year.items():\n",
    "\n",
    "    #mask to first filter out na values in zip code (we will deal with those later)\n",
    "    non_na_mask_zip = df['Zip Code'].notna()\n",
    "\n",
    "    #get rows containing non-numeric values\n",
    "    non_numeric_rows_zip = df[non_na_mask_zip & df['Zip Code'].astype(str).str.contains(pattern, regex=True, na=False)]\n",
    "\n",
    "    #store non-numeric rows in a list\n",
    "    if not non_numeric_rows_zip.empty:\n",
    "        dataframes_with_non_numeric_values.append(non_numeric_rows_zip)\n",
    "\n",
    "#concatentate list containing non-numeric rows together in a dataframe. \n",
    "if dataframes_with_non_numeric_values:\n",
    "    concatenated_dunlap_df = pd.concat(dataframes_with_non_numeric_values)\n",
    "    print(\"Non-numeric values detected in 'Zip Code' column in yearly dataframes (excluding NA values):\")\n",
    "else:\n",
    "    print(\"No rows with non-numeric values detected in 'Zip Code' column (excluding NA values).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the value counts for 'Zip Code' and 'City', we see that there was an error in entering the data for the city of 'DUNLAP'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Zip Code\n",
       "712-2    7940\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_dunlap_df['Zip Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "DUNLAP    7940\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_dunlap_df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to loop over the 'Item Number' column to detect mismatched datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values detected in 'Item Number' column in yearly dataframes (excluding NA values):\n"
     ]
    }
   ],
   "source": [
    "#regex pattern to find rows with non-numeric values(excudling '.')\n",
    "pattern = r'[^0-9\\.]' \n",
    "\n",
    "#list to store dfs with non_numeric values\n",
    "dataframes_with_non_numeric_values_item = []\n",
    "\n",
    "#loop to iterate through yearly dataframes over 'Item Number'\n",
    "for df_name, df in dataframes_by_year.items():\n",
    "     #mask to first filter out na values in item number\n",
    "    non_na_mask_item = df['Item Number'].notna()\n",
    "\n",
    "    #get rows containing non-numeric values\n",
    "    non_numeric_rows_item = df[non_na_mask_item & df['Item Number'].astype(str).str.contains(pattern, regex=True, na=False)]\n",
    "\n",
    "    #store non-numeric rows in a list\n",
    "    if not non_numeric_rows_item.empty:\n",
    "        dataframes_with_non_numeric_values_item.append(non_numeric_rows_item)\n",
    "\n",
    "#concatentate list containing non-numeric rows together in a dataframe. \n",
    "if dataframes_with_non_numeric_values_item:\n",
    "    concatenated_df_bad_item = pd.concat(dataframes_with_non_numeric_values_item)\n",
    "    print(\"Non-numeric values detected in 'Item Number' column in yearly dataframes (excluding NA values):\")\n",
    "else:\n",
    "    print(\"No rows with non-numeric values detected in 'Item Number' column (excluding NA values).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Store Number</th>\n",
       "      <th>Store Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Store Location</th>\n",
       "      <th>County Number</th>\n",
       "      <th>County</th>\n",
       "      <th>Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Item Number</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Pack</th>\n",
       "      <th>Bottle Volume (ml)</th>\n",
       "      <th>State Bottle Cost</th>\n",
       "      <th>State Bottle Retail</th>\n",
       "      <th>Bottles Sold</th>\n",
       "      <th>Sale (Dollars)</th>\n",
       "      <th>Volume Sold (Liters)</th>\n",
       "      <th>Volume Sold (Gallons)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8293604</th>\n",
       "      <td>10/11/2016</td>\n",
       "      <td>2619</td>\n",
       "      <td>HY-VEE WINE AND SPIRITS / WDM</td>\n",
       "      <td>1725  74TH ST</td>\n",
       "      <td>WEST DES MOINES</td>\n",
       "      <td>50266</td>\n",
       "      <td>POINT (-93.808855 41.598515)</td>\n",
       "      <td>77.0</td>\n",
       "      <td>POLK</td>\n",
       "      <td>1901200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>x904631</td>\n",
       "      <td>TANQUERAY GIN MINI - USE 904631 CODE</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>14.55</td>\n",
       "      <td>21.83</td>\n",
       "      <td>12</td>\n",
       "      <td>261.96</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9299557</th>\n",
       "      <td>04/06/2017</td>\n",
       "      <td>4988</td>\n",
       "      <td>HAPPY'S WINE &amp; SPIRITS</td>\n",
       "      <td>5925 UNIVERSITY AVE</td>\n",
       "      <td>CEDAR FALLS</td>\n",
       "      <td>50613.0</td>\n",
       "      <td>POINT (-92.429331 42.512766)</td>\n",
       "      <td>7.0</td>\n",
       "      <td>BLACK HAWK</td>\n",
       "      <td>1901200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>x904631</td>\n",
       "      <td>TANQUERAY GIN MINI - USE 904631 CODE</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>14.55</td>\n",
       "      <td>21.83</td>\n",
       "      <td>12</td>\n",
       "      <td>261.96</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319467</th>\n",
       "      <td>04/04/2017</td>\n",
       "      <td>2665</td>\n",
       "      <td>HY-VEE / WAUKEE</td>\n",
       "      <td>1005 E HICKMAN RD</td>\n",
       "      <td>WAUKEE</td>\n",
       "      <td>50263.0</td>\n",
       "      <td>POINT (-93.854477 41.615059)</td>\n",
       "      <td>25.0</td>\n",
       "      <td>DALLAS</td>\n",
       "      <td>1901200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>x904631</td>\n",
       "      <td>TANQUERAY GIN MINI - USE 904631 CODE</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>14.55</td>\n",
       "      <td>21.83</td>\n",
       "      <td>12</td>\n",
       "      <td>261.96</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date  Store Number                     Store Name  \\\n",
       "8293604  10/11/2016          2619  HY-VEE WINE AND SPIRITS / WDM   \n",
       "9299557  04/06/2017          4988         HAPPY'S WINE & SPIRITS   \n",
       "9319467  04/04/2017          2665                HY-VEE / WAUKEE   \n",
       "\n",
       "                     Address             City Zip Code  \\\n",
       "8293604        1725  74TH ST  WEST DES MOINES    50266   \n",
       "9299557  5925 UNIVERSITY AVE      CEDAR FALLS  50613.0   \n",
       "9319467    1005 E HICKMAN RD           WAUKEE  50263.0   \n",
       "\n",
       "                       Store Location  County Number      County   Category  \\\n",
       "8293604  POINT (-93.808855 41.598515)           77.0        POLK  1901200.0   \n",
       "9299557  POINT (-92.429331 42.512766)            7.0  BLACK HAWK  1901200.0   \n",
       "9319467  POINT (-93.854477 41.615059)           25.0      DALLAS  1901200.0   \n",
       "\n",
       "         ... Item Number                      Item Description Pack  \\\n",
       "8293604  ...     x904631  TANQUERAY GIN MINI - USE 904631 CODE   12   \n",
       "9299557  ...     x904631  TANQUERAY GIN MINI - USE 904631 CODE   12   \n",
       "9319467  ...     x904631  TANQUERAY GIN MINI - USE 904631 CODE   12   \n",
       "\n",
       "        Bottle Volume (ml) State Bottle Cost  State Bottle Retail  \\\n",
       "8293604                500             14.55                21.83   \n",
       "9299557                500             14.55                21.83   \n",
       "9319467                500             14.55                21.83   \n",
       "\n",
       "         Bottles Sold  Sale (Dollars)  Volume Sold (Liters)  \\\n",
       "8293604            12          261.96                   6.0   \n",
       "9299557            12          261.96                   6.0   \n",
       "9319467            12          261.96                   6.0   \n",
       "\n",
       "         Volume Sold (Gallons)  \n",
       "8293604                   1.59  \n",
       "9299557                   1.59  \n",
       "9319467                   1.59  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df_bad_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, there are exactly 3 data points which have 'x904631' instead of a numeric value.\n",
    "\n",
    "Now we will loop through and correct these issues. We will simply replace any values in the Zip Code column containing '712-2' with 51529 since they only occur in rows where the City is DUNLAP. Then we will the remove the tiny number of rows containing non-numeric data types in the 'Item Number' column for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: Iowa_Liquor_Data_2012\n",
      "Length before processing: 2082059\n",
      "rows before zip code conversion: 1485\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2082059\n",
      "Processing DataFrame: Iowa_Liquor_Data_2013\n",
      "Length before processing: 2063763\n",
      "rows before zip code conversion: 1614\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2063763\n",
      "Processing DataFrame: Iowa_Liquor_Data_2014\n",
      "Length before processing: 2097796\n",
      "rows before zip code conversion: 1780\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2097796\n",
      "Processing DataFrame: Iowa_Liquor_Data_2015\n",
      "Length before processing: 2184483\n",
      "rows before zip code conversion: 1725\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2184483\n",
      "Processing DataFrame: Iowa_Liquor_Data_2016\n",
      "Length before processing: 2279893\n",
      "rows before zip code conversion: 1336\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2279892\n",
      "Processing DataFrame: Iowa_Liquor_Data_2017\n",
      "Length before processing: 2291276\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2291274\n",
      "Processing DataFrame: Iowa_Liquor_Data_2018\n",
      "Length before processing: 2355556\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2355556\n",
      "Processing DataFrame: Iowa_Liquor_Data_2019\n",
      "Length before processing: 2380345\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2380345\n",
      "Processing DataFrame: Iowa_Liquor_Data_2020\n",
      "Length before processing: 2614365\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2614365\n",
      "Processing DataFrame: Iowa_Liquor_Data_2021\n",
      "Length before processing: 2622712\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2622712\n",
      "Processing DataFrame: Iowa_Liquor_Data_2022\n",
      "Length before processing: 2564565\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 2564565\n",
      "Processing DataFrame: Iowa_Liquor_Data_2023\n",
      "Length before processing: 1952928\n",
      "rows before zip code conversion: 0\n",
      "rows containing bad zip after zip code conversion: 0\n",
      "Length after processing: 1952928\n",
      "\n",
      "Total removed rows where 'Item Number' == 'x904631': 3\n",
      "Total changed 'Zip Code' values: 7940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#counters to track removed rows for bad item numbers and number of zip code values changed\n",
    "bad_inum_removed_rows_counter = 0\n",
    "changed_zip_code_counter = 0\n",
    "\n",
    "#loop through dataframes in year dataframes list\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"Processing DataFrame: {name}\")\n",
    "    print(f\"Length before processing: {len(df)}\")\n",
    "    \n",
    "    #count rows before the conversion\n",
    "    rows_before_conversion = df[df['Zip Code'] == '712-2'].shape[0]\n",
    "    print(f\"rows before zip code conversion: {rows_before_conversion}\")\n",
    "\n",
    "\n",
    "    #change 'Zip Code' value '712-2' to 51529\n",
    "    df.loc[df['Zip Code'] == '712-2', 'Zip Code'] = 51529\n",
    "    rows_after_conversion = df[df['Zip Code'] == '712-2'].shape[0]\n",
    "    print(f\"rows containing bad zip after zip code conversion: {rows_after_conversion}\")\n",
    "\n",
    "    total_converted = rows_before_conversion - rows_after_conversion\n",
    "    changed_zip_code_counter += total_converted\n",
    "\n",
    "    #remove rows where 'Item Number' == 'x904631' and update counter\n",
    "    removed_rows = df[df['Item Number'] == 'x904631'].index\n",
    "    bad_inum_removed_rows_counter += len(removed_rows)\n",
    "    df.drop(removed_rows, inplace=True)\n",
    "    \n",
    "    #update yearly dataframe in list\n",
    "    dataframes_by_year[name] = df\n",
    "    \n",
    "    print(f\"Length after processing: {len(df)}\")\n",
    "\n",
    "#print total number of removed rows and changed zip codes\n",
    "print(f\"\\nTotal removed rows where 'Item Number' == 'x904631': {bad_inum_removed_rows_counter}\")\n",
    "print(f\"Total changed 'Zip Code' values: {changed_zip_code_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nw I am ready to separate rows containing NaN values from the other yearly dataframes into a single dataframe. I will call this dataframe cumulative_errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Iowa_Liquor_Data_2012...\n",
      "Length before removing NaN rows: 2082059\n",
      "Length after removing NaN rows: 1884232\n",
      "Processing Iowa_Liquor_Data_2013...\n",
      "Length before removing NaN rows: 2063763\n",
      "Length after removing NaN rows: 1872144\n",
      "Processing Iowa_Liquor_Data_2014...\n",
      "Length before removing NaN rows: 2097796\n",
      "Length after removing NaN rows: 1909019\n",
      "Processing Iowa_Liquor_Data_2015...\n",
      "Length before removing NaN rows: 2184483\n",
      "Length after removing NaN rows: 1984108\n",
      "Processing Iowa_Liquor_Data_2016...\n",
      "Length before removing NaN rows: 2279892\n",
      "Length after removing NaN rows: 1962631\n",
      "Processing Iowa_Liquor_Data_2017...\n",
      "Length before removing NaN rows: 2291274\n",
      "Length after removing NaN rows: 2028321\n",
      "Processing Iowa_Liquor_Data_2018...\n",
      "Length before removing NaN rows: 2355556\n",
      "Length after removing NaN rows: 2129085\n",
      "Processing Iowa_Liquor_Data_2019...\n",
      "Length before removing NaN rows: 2380345\n",
      "Length after removing NaN rows: 2158261\n",
      "Processing Iowa_Liquor_Data_2020...\n",
      "Length before removing NaN rows: 2614365\n",
      "Length after removing NaN rows: 2377003\n",
      "Processing Iowa_Liquor_Data_2021...\n",
      "Length before removing NaN rows: 2622712\n",
      "Length after removing NaN rows: 2310998\n",
      "Processing Iowa_Liquor_Data_2022...\n",
      "Length before removing NaN rows: 2564565\n",
      "Length after removing NaN rows: 2348055\n",
      "Processing Iowa_Liquor_Data_2023...\n",
      "Length before removing NaN rows: 1952928\n",
      "Length after removing NaN rows: 1935385\n",
      "\n",
      "Length of cumulative_errors: 2590496\n"
     ]
    }
   ],
   "source": [
    "#create empty dataframe to store rows containing NA values\n",
    "cumulative_errors = pd.DataFrame()\n",
    "\n",
    "#iterate through dataframes in yearly dataframes list\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    \n",
    "    #print length before removing Na rows\n",
    "    print(f\"Length before removing NaN rows: {len(df)}\")\n",
    "    \n",
    "    #identify rows with NA values\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    \n",
    "    #concatenate nan_rows to cumulative errors\n",
    "    cumulative_errors = pd.concat([cumulative_errors, nan_rows], ignore_index=True)\n",
    "    \n",
    "    #remove nan rows from original yearly dataframe\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    #print the length after removing Na rows to ensure we are actually separating that data out.\n",
    "    print(f\"Length after removing NaN rows: {len(df)}\")\n",
    "\n",
    "#print number of rows containing na values\n",
    "print(f\"\\nLength of cumulative_errors: {len(cumulative_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let's try to convert the Zip Code and Item Number column types to ensure no lingering issues persist. After doing the conversion, we will drop rows with na values. If we don't drop any rows, then the conversion was a success. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: Iowa_Liquor_Data_2012\n",
      "Length before processing: 1884232\n",
      "No rows lost for Iowa_Liquor_Data_2012: True\n",
      "Length after processing: 1884232\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2013\n",
      "Length before processing: 1872144\n",
      "No rows lost for Iowa_Liquor_Data_2013: True\n",
      "Length after processing: 1872144\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2014\n",
      "Length before processing: 1909019\n",
      "No rows lost for Iowa_Liquor_Data_2014: True\n",
      "Length after processing: 1909019\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2015\n",
      "Length before processing: 1984108\n",
      "No rows lost for Iowa_Liquor_Data_2015: True\n",
      "Length after processing: 1984108\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2016\n",
      "Length before processing: 1962631\n",
      "No rows lost for Iowa_Liquor_Data_2016: True\n",
      "Length after processing: 1962631\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2017\n",
      "Length before processing: 2028321\n",
      "No rows lost for Iowa_Liquor_Data_2017: True\n",
      "Length after processing: 2028321\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2018\n",
      "Length before processing: 2129085\n",
      "No rows lost for Iowa_Liquor_Data_2018: True\n",
      "Length after processing: 2129085\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2019\n",
      "Length before processing: 2158261\n",
      "No rows lost for Iowa_Liquor_Data_2019: True\n",
      "Length after processing: 2158261\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2020\n",
      "Length before processing: 2377003\n",
      "No rows lost for Iowa_Liquor_Data_2020: True\n",
      "Length after processing: 2377003\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2021\n",
      "Length before processing: 2310998\n",
      "No rows lost for Iowa_Liquor_Data_2021: True\n",
      "Length after processing: 2310998\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2022\n",
      "Length before processing: 2348055\n",
      "No rows lost for Iowa_Liquor_Data_2022: True\n",
      "Length after processing: 2348055\n",
      "\n",
      "Processing DataFrame: Iowa_Liquor_Data_2023\n",
      "Length before processing: 1935385\n",
      "No rows lost for Iowa_Liquor_Data_2023: True\n",
      "Length after processing: 1935385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#iterate through yearly dataframes\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"Processing DataFrame: {name}\")\n",
    "\n",
    "    initial_length = len(df)\n",
    "    print(f\"Length before processing: {len(df)}\")\n",
    "\n",
    "    #convert 'Zip Code' to type int \n",
    "    df['Zip Code'] = pd.to_numeric(df['Zip Code'], errors='coerce').astype('Int64')\n",
    "\n",
    "    #convert 'Item Number' to type int\n",
    "    df['Item Number'] = pd.to_numeric(df['Item Number'], errors='coerce').astype('Int64')\n",
    "\n",
    "    #drop rows with Na values\n",
    "    df.dropna(inplace=True)\n",
    "    final_length = len(df)\n",
    "\n",
    "    len_bool = (final_length - initial_length == 0)\n",
    "\n",
    "    print(f'No rows lost for {name}: {len_bool}')\n",
    "\n",
    "    print(f\"Length after processing: {len(df)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the rows containing missing values separated, let's investigate this dataset further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2590496"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cumulative_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Store Number</th>\n",
       "      <th>Store Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Store Location</th>\n",
       "      <th>County Number</th>\n",
       "      <th>County</th>\n",
       "      <th>Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Item Number</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Pack</th>\n",
       "      <th>Bottle Volume (ml)</th>\n",
       "      <th>State Bottle Cost</th>\n",
       "      <th>State Bottle Retail</th>\n",
       "      <th>Bottles Sold</th>\n",
       "      <th>Sale (Dollars)</th>\n",
       "      <th>Volume Sold (Liters)</th>\n",
       "      <th>Volume Sold (Gallons)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/06/2012</td>\n",
       "      <td>2643</td>\n",
       "      <td>HY-VEE WINE AND SPIRITS / WATERLOO</td>\n",
       "      <td>2126 KIMBALL AVE</td>\n",
       "      <td>WATERLOO</td>\n",
       "      <td>50701.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>BLACK HAWK</td>\n",
       "      <td>1081700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76040</td>\n",
       "      <td>MIDNIGHT MOON STRAWBERRY</td>\n",
       "      <td>6</td>\n",
       "      <td>750</td>\n",
       "      <td>10.42</td>\n",
       "      <td>15.63</td>\n",
       "      <td>6</td>\n",
       "      <td>93.78</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04/25/2012</td>\n",
       "      <td>2535</td>\n",
       "      <td>HY-VEE FOOD STORE #1 / WDM</td>\n",
       "      <td>1700 VALLEY WEST DR</td>\n",
       "      <td>WEST DES MOINES</td>\n",
       "      <td>50265.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>POLK</td>\n",
       "      <td>1032080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34433</td>\n",
       "      <td>GREY GOOSE VODKA</td>\n",
       "      <td>12</td>\n",
       "      <td>750</td>\n",
       "      <td>17.97</td>\n",
       "      <td>26.96</td>\n",
       "      <td>5</td>\n",
       "      <td>134.80</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07/19/2012</td>\n",
       "      <td>3912</td>\n",
       "      <td>SMOKIN' JOE'S #14 TOBACCO AND LIQUOR</td>\n",
       "      <td>225 EDGEWOOD RD</td>\n",
       "      <td>CEDAR RAPIDS</td>\n",
       "      <td>52405.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.0</td>\n",
       "      <td>LINN</td>\n",
       "      <td>1011200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19063</td>\n",
       "      <td>JIM BEAM</td>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>2.85</td>\n",
       "      <td>4.28</td>\n",
       "      <td>6</td>\n",
       "      <td>25.68</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/05/2012</td>\n",
       "      <td>4410</td>\n",
       "      <td>KUM &amp; GO #203 / PERRY</td>\n",
       "      <td>1219 1ST ST</td>\n",
       "      <td>PERRY</td>\n",
       "      <td>50220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>BOONE</td>\n",
       "      <td>1062310.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43331</td>\n",
       "      <td>CAPTAIN MORGAN SPICED RUM MINI</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>5.50</td>\n",
       "      <td>8.25</td>\n",
       "      <td>2</td>\n",
       "      <td>16.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04/17/2012</td>\n",
       "      <td>4262</td>\n",
       "      <td>CORK AND BOTTLE / CARROLL</td>\n",
       "      <td>1004 HIGHWAY 30 W</td>\n",
       "      <td>CARROLL</td>\n",
       "      <td>51401.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CARROLL</td>\n",
       "      <td>1032080.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34578</td>\n",
       "      <td>PINNACLE VODKA</td>\n",
       "      <td>6</td>\n",
       "      <td>1750</td>\n",
       "      <td>10.43</td>\n",
       "      <td>16.14</td>\n",
       "      <td>1</td>\n",
       "      <td>16.14</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Store Number                            Store Name  \\\n",
       "0  08/06/2012          2643    HY-VEE WINE AND SPIRITS / WATERLOO   \n",
       "1  04/25/2012          2535            HY-VEE FOOD STORE #1 / WDM   \n",
       "2  07/19/2012          3912  SMOKIN' JOE'S #14 TOBACCO AND LIQUOR   \n",
       "3  11/05/2012          4410                 KUM & GO #203 / PERRY   \n",
       "4  04/17/2012          4262             CORK AND BOTTLE / CARROLL   \n",
       "\n",
       "               Address             City Zip Code Store Location  \\\n",
       "0     2126 KIMBALL AVE         WATERLOO  50701.0            NaN   \n",
       "1  1700 VALLEY WEST DR  WEST DES MOINES  50265.0            NaN   \n",
       "2      225 EDGEWOOD RD     CEDAR RAPIDS  52405.0            NaN   \n",
       "3          1219 1ST ST            PERRY  50220.0            NaN   \n",
       "4    1004 HIGHWAY 30 W          CARROLL  51401.0            NaN   \n",
       "\n",
       "   County Number      County   Category  ... Item Number  \\\n",
       "0            7.0  BLACK HAWK  1081700.0  ...       76040   \n",
       "1           77.0        POLK  1032080.0  ...       34433   \n",
       "2           57.0        LINN  1011200.0  ...       19063   \n",
       "3            8.0       BOONE  1062310.0  ...       43331   \n",
       "4           14.0     CARROLL  1032080.0  ...       34578   \n",
       "\n",
       "                 Item Description Pack Bottle Volume (ml) State Bottle Cost  \\\n",
       "0        MIDNIGHT MOON STRAWBERRY    6                750             10.42   \n",
       "1                GREY GOOSE VODKA   12                750             17.97   \n",
       "2                        JIM BEAM   48                200              2.85   \n",
       "3  CAPTAIN MORGAN SPICED RUM MINI   12                500              5.50   \n",
       "4                  PINNACLE VODKA    6               1750             10.43   \n",
       "\n",
       "   State Bottle Retail  Bottles Sold  Sale (Dollars)  Volume Sold (Liters)  \\\n",
       "0                15.63             6           93.78                  4.50   \n",
       "1                26.96             5          134.80                  3.75   \n",
       "2                 4.28             6           25.68                  1.20   \n",
       "3                 8.25             2           16.50                  1.00   \n",
       "4                16.14             1           16.14                  1.75   \n",
       "\n",
       "   Volume Sold (Gallons)  \n",
       "0                   1.19  \n",
       "1                   0.99  \n",
       "2                   0.32  \n",
       "3                   0.26  \n",
       "4                   0.46  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help orient our data cleaning strategy, let's see which columns contain NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Address', 'City', 'Zip Code', 'Store Location', 'County Number',\n",
      "       'County', 'Category', 'Category Name', 'Vendor Number', 'Vendor Name',\n",
      "       'State Bottle Cost', 'State Bottle Retail', 'Sale (Dollars)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cumulative_errors.columns[cumulative_errors.isna().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by addressing rows which contain missing data in either the State Bottle Cost, State Bottle Retail, Vendor Number, and Vendor Name columns. There are a grand total of 19 rows out of over 27 million data points that fit this criteria. For the sake of simplicity, I will remove them for now. However, all data points removed going forward (including this case), will be stored in a separate dataframe called the_pile for further refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_pile = cumulative_errors[cumulative_errors[['State Bottle Cost', 'State Bottle Retail', 'Vendor Number', 'Vendor Name']].isnull().any(axis=1)]\n",
    "len(the_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding in earlier removed rows based on bad 'Item Number' data\n",
    "the_pile = pd.concat([the_pile, concatenated_df_bad_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2590477"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This removes an entire 19 rows in the whole dataset and makes life much easier\n",
    "cumulative_errors = cumulative_errors.dropna(subset=['State Bottle Cost', 'State Bottle Retail'], how='any')\n",
    "cumulative_errors = cumulative_errors.dropna(subset=['Vendor Number', 'Vendor Name'], how='any')\n",
    "len(cumulative_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine which rows still contain NA values after removal of those rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Address', 'City', 'Zip Code', 'Store Location', 'County Number',\n",
      "       'County', 'Category', 'Category Name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cumulative_errors.columns[cumulative_errors.isna().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's address the Category, and Category Name columns. If we subset the rows which contain either a 'Category' or 'Category Name' column, we find that there are 25040 rows. This isn't ideal, but in the interest of time, I am simply going to move those rows to the_pile. The difficulty here is that 'Category' and 'Category Name' aren't always stable for the same 'Item Description', making it somewhat difficult to determine the best way of handling these missing values. Still, this data represents less than 0.1 percent of total data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25040\n"
     ]
    }
   ],
   "source": [
    "#getting value counts of 'Item Description' for rows where 'Category' or 'Category Name' are null.\n",
    "category_nan_df = cumulative_errors[cumulative_errors['Category'].isnull() | cumulative_errors['Category Name'].isnull()]\n",
    "print(len(category_nan_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25062"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding in earlier removed rows based on bad 'Item Number' data\n",
    "the_pile = pd.concat([the_pile, category_nan_df])\n",
    "len(the_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Address', 'City', 'Zip Code', 'Store Location', 'County Number',\n",
      "       'County'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#drop na rows in 'Category; and 'Category Name' columns for now\n",
    "cumulative_errors = cumulative_errors.dropna(subset=['Category', 'Category Name'], how='any')\n",
    "print(cumulative_errors.columns[cumulative_errors.isna().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the County and County Number Columns. Any rows that contain a missing County value will also contain a missing County Number column and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_nan_df = cumulative_errors[cumulative_errors['County'].isnull() & cumulative_errors['County Number'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159120"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(county_nan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity sake, let's examine the cities within this subset. If these cities span only a single county, then we can performa simple mapping to fill in those missing values. If they do not, we cannot safely map county and county number to those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_county_missing = county_nan_df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "DES MOINES         7575\n",
       "BELMOND            6072\n",
       "DAVENPORT          4672\n",
       "SIOUX CITY         3732\n",
       "GUTTENBERG         3340\n",
       "CEDAR RAPIDS       3198\n",
       "ANAMOSA            2729\n",
       "CRESCO             2423\n",
       "IOWA CITY          2295\n",
       "MASON CITY         2221\n",
       "COUNCIL BLUFFS     2208\n",
       "HAMPTON            2153\n",
       "HARLAN             2018\n",
       "DUBUQUE            1878\n",
       "CLARINDA           1866\n",
       "FORT DODGE         1779\n",
       "OTTUMWA            1609\n",
       "CORALVILLE         1484\n",
       "INDIANOLA          1399\n",
       "MARSHALLTOWN       1158\n",
       "WAVERLY            1112\n",
       "MUSCATINE          1038\n",
       "WATERLOO           1028\n",
       "BOONE               952\n",
       "ROCKWELL            920\n",
       "EVANSDALE           916\n",
       "MISSOURI VALLEY     904\n",
       "CLINTON             885\n",
       "GLENWOOD            879\n",
       "DUNLAP              865\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_county_missing.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des Moines, Cedar Rapids, Sioux City, and Davenport span multiple counties, so we unfortuantely cannot risk mapping county numbers to them. However, we can safely perform the following mapping as these cities lie entirely within their respective counties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Critera for manual mapping: population < 5000 & missing count > 1500\n",
    "\n",
    "mapping_values = {\n",
    "    'BELMOND': {'County': 'WRIGHT', 'County Number': 99.0},\n",
    "    'COUNCIL BLUFFS': {'County': 'POTTAWATTAMIE', 'County Number': 78.0},\n",
    "    'GUTTENBERG': {'County': 'CLAYTON', 'County Number': 22.0},\n",
    "    'ANAMOSA': {'County': 'JONES', 'County Number': 53.0},\n",
    "    'CRESCO': {'County': 'HOWARD', 'County Number': 45.0},\n",
    "    'HAMPTON': {'County': 'FRANKLIN', 'County Number': 35.0},\n",
    "    'HARLAN': {'County': 'SHELBY', 'County Number': 83.0},\n",
    "    'CLARINDA': {'County': 'PAGE', 'County Number': 73.0},\n",
    "    'FORT DODGE': {'County': 'WEBSTER', 'County Number': 94.0},\n",
    "    'CEDAR RAPIDS': {'County': 'LINN', 'County Number': 57.0},\n",
    "    'MASON CITY': {'County': 'CERRO GORDO', 'County Number': 17.0},\n",
    "    'IOWA CITY': {'County': 'JOHNSON', 'County Number': 52.0},\n",
    "    'DUBUQUE': {'County': 'DUBUQUE', 'County Number': 31.0},\n",
    "    'MARSHALLTOWN': {'County': 'MARSHALL', 'County Number': 64.0},\n",
    "    'WAVERLY': {'County': 'BREMER', 'County Number': 9.0},\n",
    "    'MUSCATINE': {'County': 'MUSCATINE', 'County Number': 70.0},\n",
    "    'WATERLOO': {'County': 'BLACK HAWK', 'County Number': 7.0},\n",
    "    'BOONE': {'County': 'BOONE', 'County Number': 15.0},\n",
    "    'COLUMBUS JUNCTION': {'County': 'LOUISA', 'County Number': 58.0},\n",
    "    'CORALVILLE': {'County': 'JOHNSON', 'County Number': 52.0},\n",
    "    'CLINTON': {'County': 'CLINTON', 'County Number': 23.0},\n",
    "    'INDIANOLA': {'County': 'WARREN', 'County Number': 91.0}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115071"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update values in other_rows_df based on the mapping\n",
    "for city, values in mapping_values.items():\n",
    "    cumulative_errors.loc[cumulative_errors['City'] == city, 'County'] = values['County']\n",
    "    cumulative_errors.loc[cumulative_errors['City'] == city, 'County Number'] = values['County Number']\n",
    "\n",
    "# Select rows in county_nan_df where 'County' and 'County Number' are both NaN\n",
    "county_nan_df = cumulative_errors[cumulative_errors['County'].isnull() & cumulative_errors['County Number'].isnull()]\n",
    "len(county_nan_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filling in some of those missing values, we still have a sizeable chunk that contain missing county information. However, in the interest of time, shunt the missing data to the pile for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140133"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding bad county data to the pile\n",
    "the_pile = pd.concat([the_pile, county_nan_df])\n",
    "len(the_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows that contain missing county and county number data for now\n",
    "cumulative_errors = cumulative_errors.dropna(subset=['County', 'County Number'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cumulative_errors.columns[cumulative_errors.isna().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to Address and Zip Code. If we subset the remaining data that either contain missing Address or Zip Code fields, there are a 'whopping' 23 rows. Therefore, I feel safe simply moving this data to the_pile for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_address_df = cumulative_errors[cumulative_errors['Address'].isnull() | cumulative_errors['Zip Code'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nan_address_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140156"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding bad address data to the pile\n",
    "the_pile = pd.concat([the_pile, nan_address_df])\n",
    "len(the_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows that contain missing Address or Zip Code data for now\n",
    "cumulative_errors = cumulative_errors.dropna(subset=['Address', 'Zip Code'], how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Store Location'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cumulative_errors.columns[cumulative_errors.isna().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we delve into this task of filling in geodata, we must address a significant matter. It's essential to ensure that a city linked with the Store Number in the liquor_stores data matches the corresponding city associated with the same store number in the year-specific data. In cases where these cities don't match, relying on the geodata obtained from the liquor store may compromise accuracy. We also need to check for mismatches in the Zip Code as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note: Ensuring there is a complete match between the 'Address' columns for both datasets is way too restrictice. Slight differences in the spelling of the same address could indicate a mismatch. For example, Boston St. and Boston Street are the same address, but it would be incorrect to say we can't use the geodata in the liquor_stores to fill in missing geodata. The code below shows that there are large numbers of store numbers in the liquor_stores data that have at least one mismatched address in the 20+ million rows in the yearly liquor sales data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching addresses count: 2286\n",
      "Mismatching addresses count: 207\n"
     ]
    }
   ],
   "source": [
    "#initialize counters to track whether associated store number addresses are mismatched\n",
    "matching_address_counter = 0\n",
    "mismatching_address_counter = 0\n",
    "\n",
    "#iterate through the store number in liquore stores\n",
    "for store_num in liquor_stores['Store Number'].unique():\n",
    "    #instantiate an empty set to collect addresses associated with the store number in both liquor_stores and dataframes_by_year\n",
    "    all_addresses = set()\n",
    "\n",
    "    #get the address from liquor_stores for the current store number\n",
    "    store_addresses = set(liquor_stores.loc[liquor_stores['Store Number'] == store_num, 'Address'])\n",
    "\n",
    "    #update set with new addresses\n",
    "    all_addresses.update(store_addresses)\n",
    "\n",
    "    #iterate through each dataframe in dataframes_by_year\n",
    "    for _, df in dataframes_by_year.items():\n",
    "        #first check if store number is in the current dataframe\n",
    "        if not df[df['Store Number'] == store_num].empty:\n",
    "            #get all 'addresses associated with the store number in the current dataframe\n",
    "            year_addresses = set(df.loc[df['Store Number'] == store_num, 'Address'])\n",
    "            #update set\n",
    "            all_addresses.update(year_addresses)\n",
    "\n",
    "    #if address set is equal to 1, the address listed in the liquor_stores data matches all instances in the yearly dataframes\n",
    "    if len(all_addresses) == 1:\n",
    "        matching_address_counter += 1\n",
    "    else:\n",
    "        mismatching_address_counter += 1\n",
    "\n",
    "#output counts\n",
    "print(f\"Matching addresses count: {matching_address_counter}\")\n",
    "print(f\"Mismatching addresses count: {mismatching_address_counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,  it is much more plausible to remove Store Numbers in the liquor_stores data associated with more than 1 zip code or more than 1 city in the yearly liquor sales data. First let's find 'Store Number' in liquor_stores associated with more than 1 'City' in the yearly sales data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching cities count: 2490\n",
      "Mismatching cities count: 3\n",
      "Mismatching city Store Numbers: [5619, 9911, 3822]\n"
     ]
    }
   ],
   "source": [
    "#initialize counters\n",
    "matching_city_counter = 0\n",
    "mismatching_city_counter = 0\n",
    "#list to store mismatching city store numbers\n",
    "mismatching_city_store_numbers = []\n",
    "\n",
    "#iterate through store number column in liquor stores\n",
    "for store_num in liquor_stores['Store Number'].unique():\n",
    "    #instantiate an empty set to collect cities associated with the store number in both liquor_stores and dataframes_by_year\n",
    "    all_cities = set()\n",
    "\n",
    "    #get city from liquor_stores for the current store number\n",
    "    store_cities = set(liquor_stores.loc[liquor_stores['Store Number'] == store_num, 'City'])\n",
    "    #update set with city info in liquor stores\n",
    "    all_cities.update(store_cities)\n",
    "\n",
    "    #iterate through yearly dataframes\n",
    "    for _, df in dataframes_by_year.items():\n",
    "        #check if the store number exists in the current dataframe\n",
    "        if not df[df['Store Number'] == store_num].empty:\n",
    "            #get all cities associated with the store number in the current dataframe\n",
    "            year_cities = set(df.loc[df['Store Number'] == store_num, 'City'])\n",
    "            #update set with potentially new cities\n",
    "            all_cities.update(year_cities)\n",
    "\n",
    "    #if length of set is 1, then we know the city is the same for all instances in all of the data\n",
    "    if len(all_cities) == 1:\n",
    "        matching_city_counter += 1\n",
    "        \n",
    "    else:\n",
    "        #if it is greater than 1, we need to update the counter and add that store num to a list\n",
    "        mismatching_city_counter += 1\n",
    "        mismatching_city_store_numbers.append(store_num)\n",
    "\n",
    "#output results\n",
    "print(f\"Matching cities count: {matching_city_counter}\")\n",
    "print(f\"Mismatching cities count: {mismatching_city_counter}\")\n",
    "print(f\"Mismatching city Store Numbers: {mismatching_city_store_numbers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do the exact same thing for Zip Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching zip codes count: 2481\n",
      "Mismatching zip codes count: 12\n",
      "Matching zip code Store Numbers: [4722, 5091, 9911, 5097, 3805, 2656, 4457, 4084, 5116, 6084, 2514, 4353]\n"
     ]
    }
   ],
   "source": [
    "#initialize counters\n",
    "matching_zip_counter = 0\n",
    "mismatching_zip_counter = 0\n",
    "#list to store mismatching city store numbers\n",
    "mismatching_zip_store_numbers = []\n",
    "\n",
    "#iterate through store number column in liquor stores\n",
    "for store_num in liquor_stores['Store Number'].unique():\n",
    "    #instantiate an empty set to collect zip codes associated with the store number in both liquor_stores and dataframes_by_year\n",
    "    all_zip_codes = set()\n",
    "\n",
    "    #get zip code from liquor_stores for the current store number\n",
    "    store_zip_codes = set(liquor_stores.loc[liquor_stores['Store Number'] == store_num, 'Zip Code'])\n",
    "    all_zip_codes.update(store_zip_codes)\n",
    "\n",
    "    #iterate through yearly dataframes\n",
    "    for _, df in dataframes_by_year.items():\n",
    "        #check if the store number exists in the current dataframe\n",
    "        if not df[df['Store Number'] == store_num].empty:\n",
    "            #get all zip codes associated with the store number in the current dataframe\n",
    "            year_zip_codes = set(df.loc[df['Store Number'] == store_num, 'Zip Code'])\n",
    "\n",
    "            #update set with potentially new cities\n",
    "            all_zip_codes.update(year_zip_codes)\n",
    "\n",
    "    #if length of set is 1, then we know the zip is the same for all instances in all of the data\n",
    "    if len(all_zip_codes) == 1:\n",
    "        matching_zip_counter += 1\n",
    "        \n",
    "    else:\n",
    "        #if it is greater than 1, we need to update the counter and add that store num to a list\n",
    "        mismatching_zip_counter += 1\n",
    "        mismatching_zip_store_numbers.append(store_num)\n",
    "\n",
    "#output results\n",
    "print(f\"Matching zip codes count: {matching_zip_counter}\")\n",
    "print(f\"Mismatching zip codes count: {mismatching_zip_counter}\")\n",
    "print(f\"Matching zip code Store Numbers: {mismatching_zip_store_numbers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because analyzing data at the county level is also important to our analysis, we need to verify that each of the store numbers in liquor_stores is associated with 1 and only 1 county in our yearly data before using it to fill missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching county count: 2374\n",
      "Mismatching county count: 119\n",
      "Mismatching county Store Numbers: [10088, 10278, 9904, 9941, 4067, 5288, 10276, 10225, 3855, 8804, 8802, 9952, 5286, 5336, 5025, 10286, 9949, 10291, 6124, 3461, 10184, 10282, 10266, 5447, 10166, 8807, 9944, 10151, 10108, 10272, 5906, 10277, 10079, 8811, 6166, 6346, 10290, 10156, 10281, 8803, 9953, 10247, 10128, 10285, 6222, 10235, 9948, 5947, 9943, 10092, 10265, 10273, 10228, 8810, 10047, 5201, 5943, 9950, 10289, 2656, 10027, 6213, 10163, 10264, 9050, 6216, 10210, 10274, 9939, 10242, 5104, 10236, 10110, 10280, 8809, 9942, 10246, 5424, 4944, 10182, 10284, 10131, 10169, 10086, 10222, 10031, 6347, 9947, 10173, 8805, 10279, 9940, 10226, 9951, 10123, 10041, 10287, 8801, 9935, 10262, 10275, 10230, 10237, 6281, 6192, 10268, 10271, 5709, 10218, 4320, 8800, 2694, 10087, 9945, 10283, 8806, 10238, 8808, 10119]\n"
     ]
    }
   ],
   "source": [
    "#initialize counters\n",
    "matching_county_counter = 0\n",
    "mismatching_county_counter = 0\n",
    "#list to store mismatching store numbers\n",
    "mismatching_county_store_numbers = []\n",
    "\n",
    "#iterate through store number column in liquor stores\n",
    "for store_num in liquor_stores['Store Number'].unique():\n",
    "    #instantiate an empty set to collect counties associated with the store number in dataframes_by_year\n",
    "    all_counties = set()\n",
    "\n",
    "    #iterate through yearly dataframes\n",
    "    for _, df in dataframes_by_year.items():\n",
    "        #check if the store number exists in the current dataframe\n",
    "        if not df[df['Store Number'] == store_num].empty:\n",
    "            #get all counties associated with the store number in the current dataframe\n",
    "            year_counties = set(df.loc[df['Store Number'] == store_num, 'County'])\n",
    "\n",
    "            #update set with potentially new counties\n",
    "            all_counties.update(year_counties)\n",
    "\n",
    "    #if length of set is 1, then we know the county is the same for all instances in all of the data\n",
    "    if len(all_counties) == 1:\n",
    "        matching_county_counter += 1\n",
    "        \n",
    "    else:\n",
    "        #if it is greater than 1, we need to update the counter and add that store num to a list\n",
    "        mismatching_county_counter += 1\n",
    "        mismatching_county_store_numbers.append(store_num)\n",
    "\n",
    "#output results\n",
    "print(f\"Matching county count: {matching_county_counter}\")\n",
    "print(f\"Mismatching county count: {mismatching_county_counter}\")\n",
    "print(f\"Mismatching county Store Numbers: {mismatching_county_store_numbers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine the two lists of Store Numbers associated with more than 1 zip code or city. Then we will filter rows with those store numbers out of the original liquor_stores data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge lists with store numbers that are associated with mismatched cities and zip codes\n",
    "combined_mismatching_store_numbers = set(mismatching_city_store_numbers + mismatching_zip_store_numbers + mismatching_county_store_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2514,\n",
       " 2656,\n",
       " 2694,\n",
       " 3461,\n",
       " 3805,\n",
       " 3822,\n",
       " 3855,\n",
       " 4067,\n",
       " 4084,\n",
       " 4320,\n",
       " 4353,\n",
       " 4457,\n",
       " 4722,\n",
       " 4944,\n",
       " 5025,\n",
       " 5091,\n",
       " 5097,\n",
       " 5104,\n",
       " 5116,\n",
       " 5201,\n",
       " 5286,\n",
       " 5288,\n",
       " 5336,\n",
       " 5424,\n",
       " 5447,\n",
       " 5619,\n",
       " 5709,\n",
       " 5906,\n",
       " 5943,\n",
       " 5947,\n",
       " 6084,\n",
       " 6124,\n",
       " 6166,\n",
       " 6192,\n",
       " 6213,\n",
       " 6216,\n",
       " 6222,\n",
       " 6281,\n",
       " 6346,\n",
       " 6347,\n",
       " 8800,\n",
       " 8801,\n",
       " 8802,\n",
       " 8803,\n",
       " 8804,\n",
       " 8805,\n",
       " 8806,\n",
       " 8807,\n",
       " 8808,\n",
       " 8809,\n",
       " 8810,\n",
       " 8811,\n",
       " 9050,\n",
       " 9904,\n",
       " 9911,\n",
       " 9935,\n",
       " 9939,\n",
       " 9940,\n",
       " 9941,\n",
       " 9942,\n",
       " 9943,\n",
       " 9944,\n",
       " 9945,\n",
       " 9947,\n",
       " 9948,\n",
       " 9949,\n",
       " 9950,\n",
       " 9951,\n",
       " 9952,\n",
       " 9953,\n",
       " 10027,\n",
       " 10031,\n",
       " 10041,\n",
       " 10047,\n",
       " 10079,\n",
       " 10086,\n",
       " 10087,\n",
       " 10088,\n",
       " 10092,\n",
       " 10108,\n",
       " 10110,\n",
       " 10119,\n",
       " 10123,\n",
       " 10128,\n",
       " 10131,\n",
       " 10151,\n",
       " 10156,\n",
       " 10163,\n",
       " 10166,\n",
       " 10169,\n",
       " 10173,\n",
       " 10182,\n",
       " 10184,\n",
       " 10210,\n",
       " 10218,\n",
       " 10222,\n",
       " 10225,\n",
       " 10226,\n",
       " 10228,\n",
       " 10230,\n",
       " 10235,\n",
       " 10236,\n",
       " 10237,\n",
       " 10238,\n",
       " 10242,\n",
       " 10246,\n",
       " 10247,\n",
       " 10262,\n",
       " 10264,\n",
       " 10265,\n",
       " 10266,\n",
       " 10268,\n",
       " 10271,\n",
       " 10272,\n",
       " 10273,\n",
       " 10274,\n",
       " 10275,\n",
       " 10276,\n",
       " 10277,\n",
       " 10278,\n",
       " 10279,\n",
       " 10280,\n",
       " 10281,\n",
       " 10282,\n",
       " 10283,\n",
       " 10284,\n",
       " 10285,\n",
       " 10286,\n",
       " 10287,\n",
       " 10289,\n",
       " 10290,\n",
       " 10291}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mismatching_store_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2493"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liquor_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2361"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter liquor_stores to remove rows where 'Store Number' appears in the set\n",
    "filtered_liquor_stores = liquor_stores[~liquor_stores['Store Number'].isin(combined_mismatching_store_numbers)]\n",
    "len(filtered_liquor_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fill in geodata, let's seperate out the rows containing missing store location info from the rest of the cumulative errors dataframe to prevent us from potentially overwriting data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating rows with Na only in the store location column\n",
    "nan_location_df = cumulative_errors[cumulative_errors['Store Location'].isnull() & cumulative_errors.drop('Store Location', axis=1).notnull().all(axis=1)]\n",
    "\n",
    "#select that do not occur in nan_location_df\n",
    "non_location_nan_df = cumulative_errors[~cumulative_errors.index.isin(nan_location_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2407088"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nan_location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43255"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_location_nan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply replace the missing geodata with the corresponding geodata in the liquor_stores data. At this point, we've verified that the geodata we are filling in from liquor_stores is associated with 1 and only 1 city, zip code, and county in our yearly data. The only possible error here would occur in the cases where:\n",
    "\n",
    "- A store has remained in the same city for 12 years AND\n",
    "- A store has remained within the same zip code for 12 years AND\n",
    "- A store has remained wthin the same county for 12 years BUT\n",
    "- At some point, the store has has moved to a different address within the same city, zip code, and county as before.\n",
    "\n",
    "This event should be exceedingly rare. Less than 10% of the store numbers in liquor_stores are associated with more than 1 address in our yearly data. And of those store numbers, the differences in addresses is not due to an actual difference in geolocation, but rather slight spelling differences. In addition, any actual errors would likely account for a small fraction of each store number associated with the missing data (as the newer address might be present in only a small subset of the data). Moreover, achieving such precise accuracy isn't relevant to this analysis anyway as the goal here is mostly in prototyping. However, if you plan to use the techniques in this notebook to examine liquor consumption at the 'neighborhood level', I would recommend that you either\n",
    "\n",
    "- Not fill geodata to rows where the Store Number is associated with more than 1 address AND OR\n",
    "- Validate/harmonize the address data for store numbers.\n",
    "\n",
    "Even after performing such granular steps, I doubt you would see much change, but I digress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge nan_location_df with filtered_liquor_stores based on store number\n",
    "merged_df = nan_location_df.merge(filtered_liquor_stores[['Store Number', 'Store Location']], on='Store Number', how='left')\n",
    "\n",
    "#update store location in nan_location_df with values from filtered_liquor_stores where matching store number exists\n",
    "nan_location_df.loc[:, 'Store Location'] = merged_df['Store Location_y'].fillna(nan_location_df['Store Location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a large chunk of missing data remains after the mapping, I will move this set to the_pile for now. This mapping successfully reduced na values by over 2 million data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rows containing Na values: 366669\n"
     ]
    }
   ],
   "source": [
    "#calculate the sum of remaining rows containing Na values in store Location column\n",
    "sum_of_nan_rows = nan_location_df.isna().sum(axis=1).sum()\n",
    "print(f\"Sum of rows containing Na values: {sum_of_nan_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I first remerge the cummulative_errors dataset (now called partially_cleaned_data) after some geodata was filled. Then, I re-identify which rows contain na values and add those to the_pile. The na values are dropped from partially_cleaned_data and now become fully_cleaned_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "partially_cleaned_data = pd.concat([nan_location_df, non_location_nan_df])\n",
    "\n",
    "#identify rows with missing values\n",
    "rows_with_na = partially_cleaned_data[partially_cleaned_data.isnull().any(axis=1)]\n",
    "\n",
    "#add missing rows to the pile\n",
    "the_pile = pd.concat([the_pile, rows_with_na])\n",
    "\n",
    "#remove rows with missing values\n",
    "fully_clean_data = partially_cleaned_data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get an overview of our 'fully cleaned data'. We see that there a couple of types that need to be changed: Date and Zip Code, so we will try to do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2083674 entries, 0 to 1095688\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Date                   object \n",
      " 1   Store Number           int64  \n",
      " 2   Store Name             object \n",
      " 3   Address                object \n",
      " 4   City                   object \n",
      " 5   Zip Code               object \n",
      " 6   Store Location         object \n",
      " 7   County Number          float64\n",
      " 8   County                 object \n",
      " 9   Category               float64\n",
      " 10  Category Name          object \n",
      " 11  Vendor Number          float64\n",
      " 12  Vendor Name            object \n",
      " 13  Item Number            object \n",
      " 14  Item Description       object \n",
      " 15  Pack                   int64  \n",
      " 16  Bottle Volume (ml)     int64  \n",
      " 17  State Bottle Cost      float64\n",
      " 18  State Bottle Retail    float64\n",
      " 19  Bottles Sold           int64  \n",
      " 20  Sale (Dollars)         float64\n",
      " 21  Volume Sold (Liters)   float64\n",
      " 22  Volume Sold (Gallons)  float64\n",
      "dtypes: float64(8), int64(4), object(11)\n",
      "memory usage: 381.5+ MB\n"
     ]
    }
   ],
   "source": [
    "fully_clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's convert the date and zip code in our cleaned dataset to their appropriate types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1058/3355942340.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fully_clean_data['Date'] = pd.to_datetime(fully_clean_data['Date'], errors='coerce')\n",
      "/tmp/ipykernel_1058/3355942340.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fully_clean_data['Zip Code'] = fully_clean_data['Zip Code'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "fully_clean_data['Date'] = pd.to_datetime(fully_clean_data['Date'], errors='coerce')\n",
    "fully_clean_data['Zip Code'] = fully_clean_data['Zip Code'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did our conversion work? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2083674 entries, 0 to 1095688\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   Date                   datetime64[ns]\n",
      " 1   Store Number           int64         \n",
      " 2   Store Name             object        \n",
      " 3   Address                object        \n",
      " 4   City                   object        \n",
      " 5   Zip Code               int64         \n",
      " 6   Store Location         object        \n",
      " 7   County Number          float64       \n",
      " 8   County                 object        \n",
      " 9   Category               float64       \n",
      " 10  Category Name          object        \n",
      " 11  Vendor Number          float64       \n",
      " 12  Vendor Name            object        \n",
      " 13  Item Number            object        \n",
      " 14  Item Description       object        \n",
      " 15  Pack                   int64         \n",
      " 16  Bottle Volume (ml)     int64         \n",
      " 17  State Bottle Cost      float64       \n",
      " 18  State Bottle Retail    float64       \n",
      " 19  Bottles Sold           int64         \n",
      " 20  Sale (Dollars)         float64       \n",
      " 21  Volume Sold (Liters)   float64       \n",
      " 22  Volume Sold (Gallons)  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), int64(5), object(9)\n",
      "memory usage: 381.5+ MB\n"
     ]
    }
   ],
   "source": [
    "fully_clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's iterate through our yearly dataframes and attempt to convert Date and Zip Code columns to their appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Iowa_Liquor_Data_2012...\n",
      "Conversion successful in Iowa_Liquor_Data_2012\n",
      "Processing Iowa_Liquor_Data_2013...\n",
      "Conversion successful in Iowa_Liquor_Data_2013\n",
      "Processing Iowa_Liquor_Data_2014...\n",
      "Conversion successful in Iowa_Liquor_Data_2014\n",
      "Processing Iowa_Liquor_Data_2015...\n",
      "Conversion successful in Iowa_Liquor_Data_2015\n",
      "Processing Iowa_Liquor_Data_2016...\n",
      "Conversion successful in Iowa_Liquor_Data_2016\n",
      "Processing Iowa_Liquor_Data_2017...\n",
      "Conversion successful in Iowa_Liquor_Data_2017\n",
      "Processing Iowa_Liquor_Data_2018...\n",
      "Conversion successful in Iowa_Liquor_Data_2018\n",
      "Processing Iowa_Liquor_Data_2019...\n",
      "Conversion successful in Iowa_Liquor_Data_2019\n",
      "Processing Iowa_Liquor_Data_2020...\n",
      "Conversion successful in Iowa_Liquor_Data_2020\n",
      "Processing Iowa_Liquor_Data_2021...\n",
      "Conversion successful in Iowa_Liquor_Data_2021\n",
      "Processing Iowa_Liquor_Data_2022...\n",
      "Conversion successful in Iowa_Liquor_Data_2022\n",
      "Processing Iowa_Liquor_Data_2023...\n",
      "Conversion successful in Iowa_Liquor_Data_2023\n",
      "DataFrame: Iowa_Liquor_Data_2012\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2013\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2014\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2015\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2016\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2017\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2018\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2019\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2020\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2021\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2022\n",
      "datetime64[ns]\n",
      "DataFrame: Iowa_Liquor_Data_2023\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    \n",
    "    #attempt to convert 'Date' column to date type\n",
    "    try:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        print(f\"Conversion successful in {name}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting 'Date' column in {name}: {e}\")\n",
    "\n",
    "#checking the data column types after conversion attempt\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(df['Date'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Iowa_Liquor_Data_2012...\n",
      "Conversion successful in Iowa_Liquor_Data_2012\n",
      "Processing Iowa_Liquor_Data_2013...\n",
      "Conversion successful in Iowa_Liquor_Data_2013\n",
      "Processing Iowa_Liquor_Data_2014...\n",
      "Conversion successful in Iowa_Liquor_Data_2014\n",
      "Processing Iowa_Liquor_Data_2015...\n",
      "Conversion successful in Iowa_Liquor_Data_2015\n",
      "Processing Iowa_Liquor_Data_2016...\n",
      "Conversion successful in Iowa_Liquor_Data_2016\n",
      "Processing Iowa_Liquor_Data_2017...\n",
      "Conversion successful in Iowa_Liquor_Data_2017\n",
      "Processing Iowa_Liquor_Data_2018...\n",
      "Conversion successful in Iowa_Liquor_Data_2018\n",
      "Processing Iowa_Liquor_Data_2019...\n",
      "Conversion successful in Iowa_Liquor_Data_2019\n",
      "Processing Iowa_Liquor_Data_2020...\n",
      "Conversion successful in Iowa_Liquor_Data_2020\n",
      "Processing Iowa_Liquor_Data_2021...\n",
      "Conversion successful in Iowa_Liquor_Data_2021\n",
      "Processing Iowa_Liquor_Data_2022...\n",
      "Conversion successful in Iowa_Liquor_Data_2022\n",
      "Processing Iowa_Liquor_Data_2023...\n",
      "Conversion successful in Iowa_Liquor_Data_2023\n",
      "DataFrame: Iowa_Liquor_Data_2012\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2013\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2014\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2015\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2016\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2017\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2018\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2019\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2020\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2021\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2022\n",
      "Int64\n",
      "DataFrame: Iowa_Liquor_Data_2023\n",
      "Int64\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    \n",
    "    #attempt to convert zip column to int\n",
    "    try:\n",
    "        df['Zip Code'] = pd.to_numeric(df['Zip Code'], errors='coerce')\n",
    "        print(f\"Conversion successful in {name}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting Zip Code column in {name}: {e}\")\n",
    "\n",
    "#checking zip column types after conversion attempt\n",
    "for name, df in dataframes_by_year.items():\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(df['Zip Code'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our type conversions worked, we can combine the fully_clean_data back in with the yearly sales data. We do this by extracting the year from our dataframes list using a regex expression. Then we extract all the data in fully_clean_data corresponding to that year. Finally we concatenate this filtered yearly data back in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Iowa_Liquor_Data_2012 before additions: 1884232\n",
      "Length of Iowa_Liquor_Data_2012 after concatenation: 2041722\n",
      "Length of Iowa_Liquor_Data_2013 before additions: 1872144\n",
      "Length of Iowa_Liquor_Data_2013 after concatenation: 2032977\n",
      "Length of Iowa_Liquor_Data_2014 before additions: 1909019\n",
      "Length of Iowa_Liquor_Data_2014 after concatenation: 2075143\n",
      "Length of Iowa_Liquor_Data_2015 before additions: 1984108\n",
      "Length of Iowa_Liquor_Data_2015 after concatenation: 2162691\n",
      "Length of Iowa_Liquor_Data_2016 before additions: 1962631\n",
      "Length of Iowa_Liquor_Data_2016 after concatenation: 2185028\n",
      "Length of Iowa_Liquor_Data_2017 before additions: 2028321\n",
      "Length of Iowa_Liquor_Data_2017 after concatenation: 2228494\n",
      "Length of Iowa_Liquor_Data_2018 before additions: 2129085\n",
      "Length of Iowa_Liquor_Data_2018 after concatenation: 2340661\n",
      "Length of Iowa_Liquor_Data_2019 before additions: 2158261\n",
      "Length of Iowa_Liquor_Data_2019 after concatenation: 2363351\n",
      "Length of Iowa_Liquor_Data_2020 before additions: 2377003\n",
      "Length of Iowa_Liquor_Data_2020 after concatenation: 2604466\n",
      "Length of Iowa_Liquor_Data_2021 before additions: 2310998\n",
      "Length of Iowa_Liquor_Data_2021 after concatenation: 2615041\n",
      "Length of Iowa_Liquor_Data_2022 before additions: 2348055\n",
      "Length of Iowa_Liquor_Data_2022 after concatenation: 2397957\n",
      "Length of Iowa_Liquor_Data_2023 before additions: 1935385\n",
      "Length of Iowa_Liquor_Data_2023 after concatenation: 1935385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1058/1274342344.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  concatenated_df = pd.concat([df, filtered_clean_data_yearly])\n"
     ]
    }
   ],
   "source": [
    "#iterate through dataframes_by_year\n",
    "for name, df in dataframes_by_year.items():\n",
    "\n",
    "    print(f\"Length of {name} before additions: {len(df)}\")\n",
    "    #extract the year from the df's name\n",
    "    year = re.findall(r'\\d{4}', name)[0]  \n",
    "    \n",
    "    #filter fully_cleaned_data based on the year\n",
    "    filtered_clean_data_yearly = fully_clean_data[fully_clean_data['Date'].dt.year == int(year)]\n",
    "    \n",
    "    #concatenate filtered_clean_data with the original DataFrame\n",
    "    concatenated_df = pd.concat([df, filtered_clean_data_yearly])\n",
    "    \n",
    "    #update dataframes_by_year with the concatenated DataFrame\n",
    "    dataframes_by_year[name] = concatenated_df\n",
    "\n",
    "    print(f\"Length of {name} after concatenation: {len(concatenated_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sum up the number of rows in our dataframes to ensure no data loss. Then let's subtract this value from the length of our original liquor_data. Next, we will subtract the length of the_pile and the length of the Colorado Springs data we removed earlier(2 data points). If this result is 0, no data was lost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of lengths of all dataframes is: 26982916\n"
     ]
    }
   ],
   "source": [
    "total_length = 0\n",
    "\n",
    "#iterate through dataframes_by_year and sum their lengths\n",
    "for df_name, df in dataframes_by_year.items():\n",
    "    total_length += len(df)\n",
    "\n",
    "# Display the total length\n",
    "print(f\"The sum of lengths of all dataframes is: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liquor_data) - total_length - len(the_pile) -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finally completed the first round of preprocessing. Now we can simply save the yearly dataframes as their own csv files. It should be noted that I am declining to concatenate all the data together here as that has a tendency to crash the kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Iowa_Liquor_Data_2012 has been saved to Iowa_Liquor_Data_2012.csv\n",
      "Dataframe Iowa_Liquor_Data_2013 has been saved to Iowa_Liquor_Data_2013.csv\n",
      "Dataframe Iowa_Liquor_Data_2014 has been saved to Iowa_Liquor_Data_2014.csv\n",
      "Dataframe Iowa_Liquor_Data_2015 has been saved to Iowa_Liquor_Data_2015.csv\n",
      "Dataframe Iowa_Liquor_Data_2016 has been saved to Iowa_Liquor_Data_2016.csv\n",
      "Dataframe Iowa_Liquor_Data_2017 has been saved to Iowa_Liquor_Data_2017.csv\n",
      "Dataframe Iowa_Liquor_Data_2018 has been saved to Iowa_Liquor_Data_2018.csv\n",
      "Dataframe Iowa_Liquor_Data_2019 has been saved to Iowa_Liquor_Data_2019.csv\n",
      "Dataframe Iowa_Liquor_Data_2020 has been saved to Iowa_Liquor_Data_2020.csv\n",
      "Dataframe Iowa_Liquor_Data_2021 has been saved to Iowa_Liquor_Data_2021.csv\n",
      "Dataframe Iowa_Liquor_Data_2022 has been saved to Iowa_Liquor_Data_2022.csv\n",
      "Dataframe Iowa_Liquor_Data_2023 has been saved to Iowa_Liquor_Data_2023.csv\n"
     ]
    }
   ],
   "source": [
    "# Loop through dataframes_by_year and save each dataframe to a CSV file\n",
    "for df_name, df in dataframes_by_year.items():\n",
    "    file_name = f\"{df_name}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Dataframe {df_name} has been saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks of code will help with the next stages of preprocessing. First, we need to get a list of the category names to later create boolean columns. We also output the_pile to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = liquor_data[['Category Name']].copy()\n",
    "category_data = cat_data.value_counts()\n",
    "#category_data.to_csv('categories.csv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_pile.to_csv('the_pile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506825"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(the_pile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
